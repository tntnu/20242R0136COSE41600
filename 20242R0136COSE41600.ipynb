{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "qnrdFsEUfJT2",
        "SpFXEcatp--c"
      ],
      "authorship_tag": "ABX9TyMLnn6nVlgtSLbum7QiHzmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tntnu/20242R0136COSE41600/blob/main/20242R0136COSE41600.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s30fOtK9TWgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26047664-75fb-488d-83ad-db48a8f19df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install open3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0DzwqADok0h",
        "outputId": "916509ca-8e07-403b-f323-31005139462c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.26.4)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.1.3)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (11.0.0)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from open3d) (1.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.6)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (3.0.3)\n",
            "Collecting werkzeug>=2.2.3 (from open3d)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (75.1.0)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21->open3d) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (3.0.2)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (1.9.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.21.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.6)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, werkzeug, retrying, pyquaternion, jedi, configargparse, comm, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.1.5 jedi-0.19.2 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6 widgetsnbextension-4.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 계층적 샘플링"
      ],
      "metadata": {
        "id": "qnrdFsEUfJT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_files_by_behavior(folders):\n",
        "    \"\"\"\n",
        "    각 행동(폴더)에서 PCD 파일을 분류하여 반환.\n",
        "    \"\"\"\n",
        "    behavior_files = defaultdict(list)\n",
        "    for folder in folders:\n",
        "        behavior_name = os.path.basename(os.path.dirname(folder))  # 폴더 이름을 행동 이름으로 사용\n",
        "        for file_name in os.listdir(folder):\n",
        "            if file_name.endswith('.pcd'):\n",
        "                file_path = os.path.join(folder, file_name)\n",
        "                behavior_files[behavior_name].append(file_path)\n",
        "    return behavior_files\n",
        "\n",
        "def stratified_sampling(behavior_files, behavior_ratios):\n",
        "    \"\"\"\n",
        "    각 행동별로 설정된 비율로 데이터를 샘플링.\n",
        "    \"\"\"\n",
        "    sampled_files = set()  # 중복 방지를 위해 집합(set) 사용\n",
        "    for behavior, files in behavior_files.items():\n",
        "        sample_ratio = behavior_ratios.get(behavior, 0.3)  # 비율이 지정되지 않은 경우 기본값 0.3\n",
        "        sample_size = int(len(files) * sample_ratio)  # 행동별 샘플링 개수\n",
        "        sampled_behavior_files = random.sample(files, min(sample_size, len(files)))  # 샘플링\n",
        "        sampled_files.update(sampled_behavior_files)  # 집합에 추가 (중복 방지)\n",
        "        print(f\"[{behavior}] 총 {len(files)}개 중 {len(sampled_behavior_files)}개 샘플링\")\n",
        "    return list(sampled_files)  # 최종 결과를 리스트로 반환\n",
        "\n",
        "def save_sampled_files(sampled_files, output_folder):\n",
        "    \"\"\"\n",
        "    샘플링된 파일을 지정된 폴더에 저장.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_path in sampled_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        shutil.copy(file_path, os.path.join(output_folder, file_name))"
      ],
      "metadata": {
        "id": "yhgkNdeLfIs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 행동별 PCD 파일이 저장된 폴더 리스트\n",
        "folders = [\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/01_straight_walk/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/02_straight_duck_walk/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/03_straight_crawl/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/04_zigzag_walk/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/05_straight_duck_walk/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/06_straight_crawl/pcd\",\n",
        "    \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/data/07_straight_walk/pcd\"\n",
        "]\n",
        "\n",
        "# 행동별 샘플링 비율 설정\n",
        "behavior_ratios = {\n",
        "    \"01_straight_walk\": 0.6,\n",
        "    \"02_straight_duck_walk\": 0.5,\n",
        "    \"03_straight_crawl\": 0.3,\n",
        "    \"04_zigzag_walk\": 0.5,\n",
        "    \"05_straight_duck_walk\": 0.5,\n",
        "    \"06_straight_crawl\": 0.4,\n",
        "    \"07_straight_walk\": 0.5\n",
        "}"
      ],
      "metadata": {
        "id": "ubLw0M01fdCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 행동별 파일 리스트 가져오기\n",
        "behavior_files = get_files_by_behavior(folders)\n",
        "\n",
        "# 각 행동별 파일 수 출력 (디버깅용)\n",
        "for behavior, files in behavior_files.items():\n",
        "    print(f\"[{behavior}] 총 파일 수: {len(files)}\")"
      ],
      "metadata": {
        "id": "neq-cqHmffg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b9859a-e3cb-4276-bc81-a77e7146baf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01_straight_walk] 총 파일 수: 288\n",
            "[02_straight_duck_walk] 총 파일 수: 549\n",
            "[03_straight_crawl] 총 파일 수: 1260\n",
            "[04_zigzag_walk] 총 파일 수: 364\n",
            "[05_straight_duck_walk] 총 파일 수: 577\n",
            "[06_straight_crawl] 총 파일 수: 774\n",
            "[07_straight_walk] 총 파일 수: 443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 계층적 샘플링 수행\n",
        "sampled_files = stratified_sampling(behavior_files, behavior_ratios)\n",
        "\n",
        "print(\"\\n샘플링된 총 파일 수:\", len(sampled_files))"
      ],
      "metadata": {
        "id": "Qaiu708hfhYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fb651c-53ab-4991-9f3d-ab4f1d2be2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01_straight_walk] 총 288개 중 172개 샘플링\n",
            "[02_straight_duck_walk] 총 549개 중 274개 샘플링\n",
            "[03_straight_crawl] 총 1260개 중 378개 샘플링\n",
            "[04_zigzag_walk] 총 364개 중 182개 샘플링\n",
            "[05_straight_duck_walk] 총 577개 중 288개 샘플링\n",
            "[06_straight_crawl] 총 774개 중 309개 샘플링\n",
            "[07_straight_walk] 총 443개 중 221개 샘플링\n",
            "\n",
            "샘플링된 총 파일 수: 1824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_sampled_files(sampled_files, output_folder):\n",
        "    \"\"\"\n",
        "    샘플링된 파일을 지정된 폴더에 저장하며 파일 이름이 고유하도록 수정.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file_path in sampled_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        behavior_name = os.path.basename(os.path.dirname(file_path))  # 폴더 이름 가져오기\n",
        "        unique_file_name = f\"{behavior_name}_{file_name}\"  # 고유 이름 생성\n",
        "        save_path = os.path.join(output_folder, unique_file_name)\n",
        "        shutil.copy(file_path, save_path)\n"
      ],
      "metadata": {
        "id": "pDwUdj0Cuz3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플링된 파일 저장\n",
        "output_folder = \"/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/sampled_pcd_files\"\n",
        "save_sampled_files(sampled_files, output_folder)\n",
        "\n",
        "# 저장된 파일 개수 출력\n",
        "saved_files_count = len(os.listdir(output_folder))\n",
        "print(f\"저장된 파일 개수: {saved_files_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXjTc6IAu5qa",
        "outputId": "4ddfe27a-0763-4029-eb70-a694beb135b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 파일 개수: 1630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최적의 파라미터"
      ],
      "metadata": {
        "id": "B4JI228Uo7nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/drive/MyDrive/ColabNotebooks/sampled_pcd_files.zip'\n",
        "\n",
        "# 3. 압축 파일 풀기\n",
        "import zipfile\n",
        "\n",
        "# 압축을 풀 디렉토리 설정\n",
        "extracted_folder = '/content/drive/MyDrive/ColabNotebooks/optimal/'\n",
        "\n",
        "# 압축 풀기\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "print(f\"압축 해제된 파일이 {extracted_folder}에 저장되었습니다.\")\n",
        "\n",
        "# 4. 압축 풀린 파일 목록 확인\n",
        "import os\n",
        "extracted_files = os.listdir(extracted_folder)\n",
        "print(\"압축 풀린 파일 목록:\")\n",
        "print(extracted_files)\n",
        "\n",
        "# 5. 예시로 첫 번째 파일을 Open3D로 읽어보기\n",
        "import open3d as o3d\n",
        "\n",
        "# 예시로 첫 번째 .pcd 파일을 읽어봄\n",
        "for filename in extracted_files:\n",
        "    if filename.endswith('.pcd'):\n",
        "        pcd_path = os.path.join(extracted_folder, filename)\n",
        "        print(f\"Processing {pcd_path}\")\n",
        "        pcd = o3d.io.read_point_cloud(pcd_path)\n",
        "        o3d.visualization.draw_geometries([pcd])\n",
        "        break  # 첫 번째 파일만 처리 후 종료"
      ],
      "metadata": {
        "id": "9cxCrSVU4YFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "\n",
        "# PCD 파일 로드 및 voxel 다운샘플링 함수\n",
        "def voxel_down_sample(pcd, voxel_size):\n",
        "    return pcd.voxel_down_sample(voxel_size)\n",
        "\n",
        "def find_optimal_voxel_size_for_all(pcd_files, pcd_directory):\n",
        "    # 테스트할 voxel 크기 범위 정의\n",
        "    voxel_sizes = np.arange(0.01, 0.1, 0.01)  # 0.01부터 0.1까지 0.01 간격\n",
        "    voxel_size_scores = []\n",
        "\n",
        "    # 각 voxel 크기에 대해 모든 PCD 파일을 테스트\n",
        "    for voxel_size in voxel_sizes:\n",
        "        total_points_after_downsampling = 0\n",
        "\n",
        "        for pcd_file in pcd_files:\n",
        "            pcd_path = os.path.join(pcd_directory, pcd_file)\n",
        "            pcd = o3d.io.read_point_cloud(pcd_path)\n",
        "\n",
        "            # 다운샘플링\n",
        "            downsampled_pcd = voxel_down_sample(pcd, voxel_size)\n",
        "\n",
        "            # 다운샘플링 후 포인트 개수 누적\n",
        "            total_points_after_downsampling += len(downsampled_pcd.points)\n",
        "\n",
        "        # 평균 포인트 개수 계산\n",
        "        avg_points = total_points_after_downsampling / len(pcd_files)\n",
        "        voxel_size_scores.append((voxel_size, avg_points))\n",
        "\n",
        "    # 평균 포인트 개수가 가장 적은 voxel size 선택\n",
        "    optimal_voxel_size = min(voxel_size_scores, key=lambda x: x[1])[0]\n",
        "    return optimal_voxel_size\n",
        "\n",
        "# PCD 파일이 있는 디렉토리 경로\n",
        "pcd_directory = \"/content/drive/MyDrive/ColabNotebooks/optimal/sampled_pcd_files\"\n",
        "\n",
        "# 디렉토리 내 모든 PCD 파일 목록\n",
        "pcd_files = [f for f in os.listdir(pcd_directory) if f.endswith('.pcd')]\n",
        "\n",
        "# PCD 파일이 존재하는지 확인\n",
        "if not pcd_files:\n",
        "    print(\"Error: No .pcd files found in the specified directory.\")\n",
        "else:\n",
        "    # 모든 파일에서 최적의 voxel size 계산\n",
        "    optimal_voxel_size = find_optimal_voxel_size_for_all(pcd_files, pcd_directory)\n",
        "\n",
        "    # 최적의 voxel size 출력\n",
        "    print(f\"Final chosen optimal voxel size: {optimal_voxel_size}\")\n",
        "\n",
        "# Final chosen optimal voxel size: 0.09\n"
      ],
      "metadata": {
        "id": "LZwhV2wN4asc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "\n",
        "\n",
        "def find_optimal_voxel_size(pcd, voxel_sizes):\n",
        "    \"\"\"\n",
        "    주어진 Voxel Size 리스트에서 최적의 값을 찾습니다.\n",
        "    최적의 기준: 다운샘플링 후 포인트 개수가 가장 작으면서 포인트가 유지되는 값\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for voxel_size in voxel_sizes:\n",
        "        downsample_pcd = pcd.voxel_down_sample(voxel_size)\n",
        "        results.append((voxel_size, len(downsample_pcd.points)))\n",
        "\n",
        "    # 포인트 개수가 최소가 되는 voxel_size 반환\n",
        "    optimal_voxel_size = min(results, key=lambda x: x[1])\n",
        "    return optimal_voxel_size\n",
        "\n",
        "\n",
        "def find_optimal_ror(pcd, nb_points_list, radius_list):\n",
        "    \"\"\"\n",
        "    주어진 ROR 파라미터(nb_points, radius) 조합에서 최적의 값을 찾습니다.\n",
        "    최적의 기준: 필터링 후 남은 포인트 개수가 가장 많은 조합\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for nb_points in nb_points_list:\n",
        "        for radius in radius_list:\n",
        "            cl, ind = pcd.remove_radius_outlier(nb_points=nb_points, radius=radius)\n",
        "            filtered_pcd = pcd.select_by_index(ind)\n",
        "            results.append(((nb_points, radius), len(filtered_pcd.points)))\n",
        "\n",
        "    # 포인트 개수가 최대가 되는 nb_points, radius 조합 반환\n",
        "    optimal_ror = max(results, key=lambda x: x[1])\n",
        "    return optimal_ror\n",
        "\n",
        "\n",
        "# PCD 파일 경로\n",
        "pcd_file_path = \"/content/drive/MyDrive/ColabNotebooks/optimal/sampled_pcd_files\"\n",
        "\n",
        "# PCD 파일 로드\n",
        "if os.path.exists(pcd_file_path):\n",
        "    original_pcd = o3d.io.read_point_cloud(pcd_file_path)\n",
        "else:\n",
        "    raise FileNotFoundError(f\"PCD file not found: {pcd_file_path}\")\n",
        "\n",
        "# Voxel Size 탐색\n",
        "voxel_sizes = np.arange(0.01, 0.2, 0.01)  # 테스트할 Voxel Size 리스트\n",
        "optimal_voxel_size, downsample_points = find_optimal_voxel_size(original_pcd, voxel_sizes)\n",
        "print(f\"Optimal Voxel Size: {optimal_voxel_size:.2f}, Points After Downsampling: {downsample_points}\")\n",
        "\n",
        "# ROR 탐색\n",
        "nb_points_list = range(5, 20, 2)  # nb_points 범위\n",
        "radius_list = np.arange(0.5, 2.0, 0.1)  # radius 범위\n",
        "optimal_ror, filtered_points = find_optimal_ror(original_pcd, nb_points_list, radius_list)\n",
        "print(f\"Optimal ROR Parameters: nb_points={optimal_ror[0]}, radius={optimal_ror[1]:.2f}, Points After Filtering: {filtered_points}\")\n",
        "\n",
        "# Optimal Voxel Size: 0.01, Optimal ROR Parameters: nb_points=5, radius=0.50\n"
      ],
      "metadata": {
        "id": "pA4Z4SP6qy1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eBbWYCHZqyyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "shdhEMNwCcPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# PCD 파일이 저장된 폴더 경로\n",
        "pcd_folder = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\sampled_pcd_files\"\n",
        "output_folder = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\processed_pcd_results\"\n",
        "\n",
        "# 결과 저장 폴더 생성\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# 전처리 및 바운딩 박스 생성 함수\n",
        "def process_pcd(file_path):\n",
        "    try:\n",
        "        # PCD 파일 읽기\n",
        "        original_pcd = o3d.io.read_point_cloud(file_path)\n",
        "\n",
        "        # Voxel Downsampling\n",
        "        voxel_size = 0.2\n",
        "        downsample_pcd = original_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
        "\n",
        "        # Radius Outlier Removal (ROR)\n",
        "        cl, ind = downsample_pcd.remove_radius_outlier(nb_points=6, radius=1.2)\n",
        "        ror_pcd = downsample_pcd.select_by_index(ind)\n",
        "\n",
        "        # RANSAC 평면 분리\n",
        "        plane_model, inliers = ror_pcd.segment_plane(distance_threshold=0.1, ransac_n=3, num_iterations=2000)\n",
        "\n",
        "        # 평면 제외 포인트 추출\n",
        "        final_point = ror_pcd.select_by_index(inliers, invert=True)\n",
        "\n",
        "        # DBSCAN 클러스터링\n",
        "        labels = np.array(final_point.cluster_dbscan(eps=0.3, min_points=10, print_progress=False))\n",
        "\n",
        "        # 노이즈 포인트는 검정색, 클러스터 포인트는 파란색으로 설정\n",
        "        colors = np.zeros((len(labels), 3))  # 기본 검정색\n",
        "        colors[labels >= 0] = [0, 0, 1]  # 파란색\n",
        "        final_point.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "        # 필터링 기준\n",
        "        min_points_in_cluster = 5\n",
        "        max_points_in_cluster = 40\n",
        "        min_z_value = -1.5\n",
        "        max_z_value = 2.5\n",
        "        min_height = 0.5\n",
        "        max_height = 2.0\n",
        "        max_distance = 30.0\n",
        "\n",
        "        # 조건을 만족하는 클러스터의 바운딩 박스 생성\n",
        "        bboxes = []\n",
        "        for i in range(labels.max() + 1):\n",
        "            cluster_indices = np.where(labels == i)[0]\n",
        "            if min_points_in_cluster <= len(cluster_indices) <= max_points_in_cluster:\n",
        "                cluster_pcd = final_point.select_by_index(cluster_indices)\n",
        "                points = np.asarray(cluster_pcd.points)\n",
        "                z_values = points[:, 2]\n",
        "                z_min = z_values.min()\n",
        "                z_max = z_values.max()\n",
        "                if min_z_value <= z_min and z_max <= max_z_value:\n",
        "                    height_diff = z_max - z_min\n",
        "                    if min_height <= height_diff <= max_height:\n",
        "                        distances = np.linalg.norm(points, axis=1)\n",
        "                        if distances.max() <= max_distance:\n",
        "                            bbox = cluster_pcd.get_axis_aligned_bounding_box()\n",
        "                            bbox.color = (1, 0, 0)\n",
        "                            bboxes.append(bbox)\n",
        "\n",
        "        return final_point, bboxes\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "p0-DcdE15rjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 PCD 파일 처리\n",
        "pcd_files = [f for f in os.listdir(pcd_folder) if f.endswith('.pcd')]\n",
        "\n",
        "for i, pcd_file in enumerate(pcd_files):\n",
        "    print(f\"Processing file {i + 1}/{len(pcd_files)}: {pcd_file}\")\n",
        "    file_path = os.path.join(pcd_folder, pcd_file)\n",
        "\n",
        "    # 포인트 클라우드와 바운딩 박스 추출\n",
        "    final_point, bboxes = process_pcd(file_path)\n",
        "\n",
        "    if final_point is not None and bboxes:\n",
        "        # 결과 저장\n",
        "        processed_file_path = os.path.join(output_folder, f\"processed_{pcd_file}\")\n",
        "        o3d.io.write_point_cloud(processed_file_path, final_point)\n",
        "\n",
        "        # 바운딩 박스 저장\n",
        "        bbox_file_path = os.path.join(output_folder, f\"bboxes_{pcd_file.replace('.pcd', '.txt')}\")\n",
        "        with open(bbox_file_path, 'w') as f:\n",
        "            for bbox in bboxes:\n",
        "                bbox_points = np.asarray(bbox.get_box_points())\n",
        "                for point in bbox_points:\n",
        "                    f.write(f\"{point[0]} {point[1]} {point[2]}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "    else:\n",
        "        print(f\"No valid clusters found in {pcd_file}\")\n",
        "\n",
        "print(\"Processing complete. Results are saved in:\", output_folder)\n"
      ],
      "metadata": {
        "id": "bWE9esVN5rg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = '/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/processed_pcd_results.zip'\n",
        "\n",
        "# 3. 압축 파일 풀기\n",
        "import zipfile\n",
        "\n",
        "# 압축을 풀 디렉토리 설정\n",
        "extracted_folder = '/content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/processed_pcd_results/'\n",
        "\n",
        "# 압축 풀기\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "print(f\"압축 해제된 파일이 {extracted_folder}에 저장되었습니다.\")\n",
        "\n",
        "# 4. 압축 풀린 파일 목록 확인\n",
        "import os\n",
        "extracted_files = os.listdir(extracted_folder)\n",
        "print(\"압축 풀린 파일 목록:\")\n",
        "print(extracted_files)\n",
        "\n",
        "# 5. 예시로 첫 번째 파일을 Open3D로 읽어보기\n",
        "import open3d as o3d\n",
        "\n",
        "# 예시로 첫 번째 .pcd 파일을 읽어봄\n",
        "for filename in extracted_files:\n",
        "    if filename.endswith('.pcd'):\n",
        "        pcd_path = os.path.join(extracted_folder, filename)\n",
        "        print(f\"Processing {pcd_path}\")\n",
        "        pcd = o3d.io.read_point_cloud(pcd_path)\n",
        "        o3d.visualization.draw_geometries([pcd])\n",
        "        break  # 첫 번째 파일만 처리 후 종료"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Nc_TtZ7TL6tE",
        "outputId": "d1a8fe41-2bec-4e37-919b-eb8ec0263053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 해제된 파일이 /content/drive/MyDrive/ColabNotebooks/20242R0136COSE41600/COSE416_HW1_data_v1/processed_pcd_results/에 저장되었습니다.\n",
            "압축 풀린 파일 목록:\n",
            "['processed_pcd_results']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PointNet"
      ],
      "metadata": {
        "id": "SpFXEcatp--c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import open3d as o3d\n",
        "\n",
        "# 전처리된 PCD 데이터 경로\n",
        "processed_pcd_folder = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\processed_pcd_results\"\n",
        "output_h5_file = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\pointnet_human_detection.h5\"\n",
        "\n",
        "# 최대 포인트 수 (PointNet 입력 요구사항에 맞게 조정)\n",
        "max_points = 1024\n",
        "\n",
        "# 데이터와 라벨 저장 리스트\n",
        "data = []\n",
        "labels = []  # 1: 사람, 0: 비사람 (라벨은 수동 설정 필요)\n",
        "\n",
        "# 전처리된 PCD 데이터를 불러와서 HDF5 파일로 변환\n",
        "for file_name in os.listdir(processed_pcd_folder):\n",
        "    if file_name.endswith(\".pcd\"):\n",
        "        file_path = os.path.join(processed_pcd_folder, file_name)\n",
        "\n",
        "        # Open3D를 사용하여 PCD 파일 로드\n",
        "        pcd = o3d.io.read_point_cloud(file_path)\n",
        "        points = np.asarray(pcd.points)\n",
        "\n",
        "        # 포인트 클라우드 정규화 (중심 이동 및 범위 조정)\n",
        "        centroid = np.mean(points, axis=0)\n",
        "        points -= centroid\n",
        "        max_dist = np.max(np.linalg.norm(points, axis=1))\n",
        "        points /= max_dist\n",
        "\n",
        "        # 포인트 클라우드 포인트 개수를 max_points로 조정 (패딩 또는 샘플링)\n",
        "        if len(points) > max_points:\n",
        "            indices = np.random.choice(len(points), max_points, replace=False)\n",
        "            points = points[indices]\n",
        "        elif len(points) < max_points:\n",
        "            pad = np.zeros((max_points - len(points), 3))\n",
        "            points = np.vstack((points, pad))\n",
        "\n",
        "        # 데이터를 리스트에 추가 (라벨은 수동 설정)\n",
        "        data.append(points)\n",
        "\n",
        "        # 라벨 추가 (예: \"processed_person_001.pcd\"는 사람으로 라벨링)\n",
        "        label = 1 if \"person\" in file_name.lower() else 0\n",
        "        labels.append(label)"
      ],
      "metadata": {
        "id": "zCdYvwzp5rU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy 배열로 변환\n",
        "data = np.array(data, dtype=np.float32)\n",
        "labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "# HDF5 파일로 저장\n",
        "with h5py.File(output_h5_file, \"w\") as hf:\n",
        "    hf.create_dataset(\"data\", data=data)\n",
        "    hf.create_dataset(\"label\", data=labels)\n",
        "\n",
        "print(f\"HDF5 파일 생성 완료: {output_h5_file}\")"
      ],
      "metadata": {
        "id": "BUZ4QeJf5rSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LL6iUAY9qW0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# HDF5 파일 로드\n",
        "with h5py.File(output_h5_file, \"r\") as hf:\n",
        "    data = hf[\"data\"][:]\n",
        "    labels = hf[\"label\"][:]\n",
        "\n",
        "# PyTorch 데이터셋 준비\n",
        "data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)  # (N, 3, 1024)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "dataset = TensorDataset(data, labels)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "koDmNEDaFMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PointNet(num_classes=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "tUe3EFfqFMpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 루프\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "D02pUvkPFMm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "torch.save(model.state_dict(), \"pointnet_human_detection.pth\")\n",
        "print(\"훈련 완료 및 모델 저장 완료!\")"
      ],
      "metadata": {
        "id": "xt9bFzNLFMkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가 함수\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# 평가 데이터 준비\n",
        "test_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 모델 평가\n",
        "accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "-ZG5CBxiFMiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PointNet 모델 로드\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델 로드\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PointNet(num_classes=2).to(device)\n",
        "model.load_state_dict(torch.load(\"pointnet_human_detection.pth\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "93T2rvADFMfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 PCD 파일 경로\n",
        "test_pcd_file = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\COSE416_HW1_data_v1\\\\data\\\\01_straight_walk\\\\pcd\\\\pcd_000001.pcd\"\n",
        "\n",
        "# 포인트 클라우드 데이터 로드 및 정규화\n",
        "pcd = o3d.io.read_point_cloud(test_pcd_file)\n",
        "points = np.asarray(pcd.points)"
      ],
      "metadata": {
        "id": "UlqFdE-FFciv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규화 (훈련 시 데이터와 동일하게 전처리)\n",
        "centroid = np.mean(points, axis=0)\n",
        "points -= centroid\n",
        "max_dist = np.max(np.linalg.norm(points, axis=1))\n",
        "points /= max_dist\n",
        "\n",
        "# 포인트 개수 맞추기\n",
        "max_points = 1024\n",
        "if len(points) > max_points:\n",
        "    indices = np.random.choice(len(points), max_points, replace=False)\n",
        "    points = points[indices]\n",
        "elif len(points) < max_points:\n",
        "    pad = np.zeros((max_points - len(points), 3))\n",
        "    points = np.vstack((points, pad))"
      ],
      "metadata": {
        "id": "z0GIALl1quWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 입력으로 변환\n",
        "input_tensor = torch.tensor(points, dtype=torch.float32).unsqueeze(0).permute(0, 2, 1).to(device)\n",
        "\n",
        "# 모델 예측\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    prediction = torch.argmax(output, dim=1).item()"
      ],
      "metadata": {
        "id": "gjkN2HO9qy67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 출력 및 시각화\n",
        "print(f\"Prediction: {'Human Detected' if prediction == 1 else 'No Human Detected'}\")\n",
        "\n",
        "if prediction == 1:\n",
        "    bbox = pcd.get_axis_aligned_bounding_box()\n",
        "    bbox.color = (1, 0, 0)  # 빨간색\n",
        "\n",
        "    # 헤드리스 시각화 저장\n",
        "    vis = o3d.visualization.Visualizer()\n",
        "    vis.create_window(visible=False)\n",
        "    vis.add_geometry(pcd)\n",
        "    vis.add_geometry(bbox)\n",
        "    vis.capture_screen_image(\"output_image.png\")\n",
        "    vis.destroy_window()\n",
        "    print(\"Visualization saved as output_image.png\")\n",
        "else:\n",
        "    print(\"No human detected; no visualization saved.\")"
      ],
      "metadata": {
        "id": "OLfX8tSMqy4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최적화 PointNet(모델 예측 시각화 오류)"
      ],
      "metadata": {
        "id": "yGQbRy6QW-RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import h5py\n",
        "\n",
        "# ========= PCD 처리 및 바운딩 박스 생성 ========= #\n",
        "def process_pcd(file_path):\n",
        "    try:\n",
        "        # PCD 파일 읽기\n",
        "        original_pcd = o3d.io.read_point_cloud(file_path)\n",
        "\n",
        "        # Voxel Downsampling\n",
        "        voxel_size = 0.01\n",
        "        downsample_pcd = original_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
        "\n",
        "        # Radius Outlier Removal (ROR)\n",
        "        cl, ind = downsample_pcd.remove_radius_outlier(nb_points=5, radius=0.5)\n",
        "        ror_pcd = downsample_pcd.select_by_index(ind)\n",
        "\n",
        "        # RANSAC 평면 분리\n",
        "        plane_model, inliers = ror_pcd.segment_plane(distance_threshold=0.1, ransac_n=3, num_iterations=2000)\n",
        "\n",
        "        # 평면 제외 포인트 추출\n",
        "        final_point = ror_pcd.select_by_index(inliers, invert=True)\n",
        "\n",
        "        # DBSCAN 클러스터링\n",
        "        labels = np.array(final_point.cluster_dbscan(eps=0.3, min_points=10, print_progress=False))\n",
        "\n",
        "        # 노이즈 포인트는 검정색, 클러스터 포인트는 파란색으로 설정\n",
        "        colors = np.zeros((len(labels), 3))  # 기본 검정색\n",
        "        colors[labels >= 0] = [0, 0, 1]  # 파란색\n",
        "        final_point.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "        # 필터링 기준\n",
        "        min_points_in_cluster = 5\n",
        "        max_points_in_cluster = 500\n",
        "        min_z_value = -1.5\n",
        "        max_z_value = 2.5\n",
        "        min_height = 0.5\n",
        "        max_height = 2.0\n",
        "        max_distance = 30.0\n",
        "\n",
        "        # 조건을 만족하는 클러스터의 바운딩 박스 생성\n",
        "        bboxes = []\n",
        "        for i in range(labels.max() + 1):\n",
        "            cluster_indices = np.where(labels == i)[0]\n",
        "            if min_points_in_cluster <= len(cluster_indices) <= max_points_in_cluster:\n",
        "                cluster_pcd = final_point.select_by_index(cluster_indices)\n",
        "                points = np.asarray(cluster_pcd.points)\n",
        "                z_values = points[:, 2]\n",
        "                z_min = z_values.min()\n",
        "                z_max = z_values.max()\n",
        "                if min_z_value <= z_min and z_max <= max_z_value:\n",
        "                    height_diff = z_max - z_min\n",
        "                    if min_height <= height_diff <= max_height:\n",
        "                        distances = np.linalg.norm(points, axis=1)\n",
        "                        if distances.max() <= max_distance:\n",
        "                            bbox = cluster_pcd.get_axis_aligned_bounding_box()\n",
        "                            bbox.color = (1, 0, 0)\n",
        "                            bboxes.append((bbox, cluster_pcd))\n",
        "\n",
        "        return final_point, bboxes\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# ========= PointNet 모델 정의 ========= #\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ========= 포인트 클라우드 처리 및 PointNet 테스트 ========= #\n",
        "def detect_humans_in_pcd(file_path, model, device, max_points=1024):\n",
        "    # PCD 처리\n",
        "    final_pcd, bboxes = process_pcd(file_path)\n",
        "    if final_pcd is None or not bboxes:\n",
        "        print(\"No valid clusters or bounding boxes detected.\")\n",
        "        return\n",
        "\n",
        "    # 바운딩 박스 내부 포인트 처리\n",
        "    for bbox, cluster_pcd in bboxes:\n",
        "        points = np.asarray(cluster_pcd.points)\n",
        "        if len(points) == 0:\n",
        "            continue\n",
        "\n",
        "        # 정규화\n",
        "        centroid = np.mean(points, axis=0)\n",
        "        points -= centroid\n",
        "        max_dist = np.max(np.linalg.norm(points, axis=1))\n",
        "        points /= max_dist\n",
        "\n",
        "        # 샘플링 및 패딩\n",
        "        if len(points) > max_points:\n",
        "            indices = np.random.choice(len(points), max_points, replace=False)\n",
        "            points = points[indices]\n",
        "        elif len(points) < max_points:\n",
        "            pad = np.zeros((max_points - len(points), 3))\n",
        "            points = np.vstack((points, pad))\n",
        "\n",
        "        # 모델 입력 준비\n",
        "        input_tensor = torch.tensor(points, dtype=torch.float32).unsqueeze(0).permute(0, 2, 1).to(device)\n",
        "\n",
        "        # 모델 예측\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "            prediction = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # 예측 결과 처리\n",
        "        if prediction == 1:  # 사람 탐지\n",
        "            bbox.color = (1, 0, 0)  # 빨간색\n",
        "            print(\"Human detected in bounding box.\")\n",
        "        else:\n",
        "            bbox.color = (0, 1, 0)  # 초록색\n",
        "            print(\"No human detected in bounding box.\")\n",
        "\n",
        "    # 결과 시각화\n",
        "    geometries = [final_pcd] + [bbox[0] for bbox in bboxes]\n",
        "    o3d.visualization.draw_geometries(geometries)\n",
        "\n",
        "# ========== mAP 계산 함수 ========== #\n",
        "def compute_map(h5_file, model_save_path):\n",
        "    # HDF5 파일 로드\n",
        "    with h5py.File(h5_file, \"r\") as hf:\n",
        "        data = hf[\"data\"][:]\n",
        "        labels = hf[\"label\"][:]\n",
        "\n",
        "    # 데이터 준비\n",
        "    data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(data, labels)\n",
        "    test_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # 모델 로드\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 예측값 및 실제값 저장\n",
        "    all_targets = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            scores = F.softmax(outputs, dim=1)[:, 1]  # \"Human\" 클래스의 확률\n",
        "\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_scores.extend(scores.cpu().numpy())\n",
        "\n",
        "    # Precision-Recall Curve 계산\n",
        "    precision, recall, _ = precision_recall_curve(all_targets, all_scores)\n",
        "    ap = average_precision_score(all_targets, all_scores)\n",
        "\n",
        "    # 그래프 그리기\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post', label=f\"AP={ap:.4f}\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Average Precision (AP): {ap:.4f}\")\n",
        "    return ap\n",
        "\n",
        "# ========== mAP 계산 함수 ========== #\n",
        "def compute_map(h5_file, model_save_path):\n",
        "    # HDF5 파일 로드\n",
        "    with h5py.File(h5_file, \"r\") as hf:\n",
        "        data = hf[\"data\"][:]\n",
        "        labels = hf[\"label\"][:]\n",
        "\n",
        "    # 데이터 준비\n",
        "    data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(data, labels)\n",
        "    test_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # 모델 로드\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 예측값 및 실제값 저장\n",
        "    all_targets = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            scores = F.softmax(outputs, dim=1)[:, 1]  # \"Human\" 클래스의 확률\n",
        "\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_scores.extend(scores.cpu().numpy())\n",
        "\n",
        "    # Precision-Recall Curve 계산\n",
        "    precision, recall, _ = precision_recall_curve(all_targets, all_scores)\n",
        "    ap = average_precision_score(all_targets, all_scores)\n",
        "\n",
        "    # 그래프 그리기\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post', label=f\"AP={ap:.4f}\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Average Precision (AP): {ap:.4f}\")\n",
        "    return ap\n",
        "\n",
        "\n",
        "\n",
        "# ========= 실행 코드 ========= #\n",
        "if __name__ == \"__main__\":\n",
        "    # 파일 경로 설정\n",
        "    test_pcd_file = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\COSE416_HW1_data_v1\\\\data\\\\01_straight_walk\\\\pcd\\\\pcd_000001.pcd\"\n",
        "    model_path = \"pointnet_human_detection.pth\"\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 모델 로드\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # mAP 계산\n",
        "    compute_map(output_h5_file, model_save_path)\n",
        "\n",
        "    # PCD 파일 처리 및 탐지 실행\n",
        "    detect_humans_in_pcd(test_pcd_file, model, device)\n"
      ],
      "metadata": {
        "id": "QHRuplcgXA3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최적화 PointNet (모델 예측 시각화 오류 해결 시도)"
      ],
      "metadata": {
        "id": "AH2wiOYq3-tW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 PointNet 전처리 이후부터 테스트 시각화 부분 수정\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ========== 경로 설정 ========== #\n",
        "processed_pcd_folder = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\processed_pcd_results_optimal\"  # 전처리된 PCD 파일 경로\n",
        "output_h5_file = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\pointnet_human_detection.h5\"  # 학습 데이터 저장 파일\n",
        "model_save_path = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\pointnet_human_detection.pth\"  # 학습된 모델 저장 경로\n",
        "test_pcd_file = \"C:\\\\Users\\\\tntnu\\\\Desktop\\\\COSE416_HW1_tutorial\\\\test_data\" # 테스트용 PCD 파일 경로\n",
        "\n",
        "# ========== PointNet 모델 정의 ========== #\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ========== HDF5 데이터 생성 ========== #\n",
        "def create_hdf5_from_pcd(processed_pcd_folder, output_h5_file, max_points=1024):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for file_name in os.listdir(processed_pcd_folder):\n",
        "        if file_name.endswith(\".pcd\"):\n",
        "            file_path = os.path.join(processed_pcd_folder, file_name)\n",
        "            pcd = o3d.io.read_point_cloud(file_path)\n",
        "            points = np.asarray(pcd.points)\n",
        "\n",
        "            # 정규화\n",
        "            centroid = np.mean(points, axis=0)\n",
        "            points -= centroid\n",
        "            max_dist = np.max(np.linalg.norm(points, axis=1))\n",
        "            points /= max_dist\n",
        "\n",
        "            # 샘플링\n",
        "            if len(points) > max_points:\n",
        "                indices = np.random.choice(len(points), max_points, replace=False)\n",
        "                points = points[indices]\n",
        "            elif len(points) < max_points:\n",
        "                pad = np.zeros((max_points - len(points), 3))\n",
        "                points = np.vstack((points, pad))\n",
        "\n",
        "            # 라벨링 (파일명에 따라 라벨 결정)\n",
        "            label = 1 if \"person\" in file_name.lower() else 0\n",
        "            data.append(points)\n",
        "            labels.append(label)\n",
        "\n",
        "    # Numpy 배열로 변환\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "    # HDF5 파일로 저장\n",
        "    with h5py.File(output_h5_file, \"w\") as hf:\n",
        "        hf.create_dataset(\"data\", data=data)\n",
        "        hf.create_dataset(\"label\", data=labels)\n",
        "    print(f\"HDF5 파일 생성 완료: {output_h5_file}\")\n",
        "\n",
        "# ========== PointNet 학습 ========== #\n",
        "def train_pointnet(h5_file, model_save_path, num_epochs=30, batch_size=16, learning_rate=0.001):\n",
        "    # HDF5 파일 로드\n",
        "    with h5py.File(h5_file, \"r\") as hf:\n",
        "        data = hf[\"data\"][:]\n",
        "        labels = hf[\"label\"][:]\n",
        "\n",
        "    # 데이터 준비\n",
        "    data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)  # (N, 3, 1024)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(data, labels)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 모델 초기화\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 학습 루프\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"모델 저장 완료: {model_save_path}\")\n",
        "\n",
        "# ========== 모델 평가 ========== #\n",
        "def evaluate_model(h5_file, model_save_path):\n",
        "    # HDF5 파일 로드\n",
        "    with h5py.File(h5_file, \"r\") as hf:\n",
        "        data = hf[\"data\"][:]\n",
        "        labels = hf[\"label\"][:]\n",
        "\n",
        "    # 데이터 준비\n",
        "    data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(data, labels)\n",
        "    test_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # 모델 로드\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "    # 정확도 계산\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"모델 정확도: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# ========== 테스트 ========== #\n",
        "def test_pointnet(model_save_path, test_pcd_file, max_points=1024):\n",
        "    # PCD 파일이 디렉토리일 경우, 첫 번째 PCD 파일을 선택\n",
        "    if os.path.isdir(test_pcd_file):\n",
        "        pcd_files = [f for f in os.listdir(test_pcd_file) if f.endswith('.pcd')]\n",
        "        if len(pcd_files) == 0:\n",
        "            print(\"Error: No PCD files found in the directory.\")\n",
        "            return\n",
        "        test_pcd_file = os.path.join(test_pcd_file, pcd_files[0])\n",
        "\n",
        "    # PCD 로드 및 전처리\n",
        "    pcd = o3d.io.read_point_cloud(test_pcd_file)\n",
        "\n",
        "    if len(pcd.points) == 0:\n",
        "        print(\"Error: The point cloud is empty.\")\n",
        "        return\n",
        "\n",
        "    points = np.asarray(pcd.points)\n",
        "\n",
        "    # 정규화\n",
        "    centroid = np.mean(points, axis=0)\n",
        "    points -= centroid\n",
        "    max_dist = np.max(np.linalg.norm(points, axis=1))\n",
        "    points /= max_dist\n",
        "\n",
        "    # 샘플링\n",
        "    if len(points) > max_points:\n",
        "        indices = np.random.choice(len(points), max_points, replace=False)\n",
        "        points = points[indices]\n",
        "    elif len(points) < max_points:\n",
        "        pad = np.zeros((max_points - len(points), 3))\n",
        "        points = np.vstack((points, pad))\n",
        "\n",
        "    # 모델 로드\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device, weights_only=True))\n",
        "    model.eval()\n",
        "\n",
        "    # 모델 예측\n",
        "    input_tensor = torch.tensor(points, dtype=torch.float32).unsqueeze(0).permute(0, 2, 1).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    # 예측 결과 출력\n",
        "    print(f\"Prediction: {'Human Detected' if prediction == 1 else 'No Human Detected'}\")\n",
        "\n",
        "    # 포인트 클라우드 시각화\n",
        "    pcd.points = o3d.utility.Vector3dVector(points)\n",
        "\n",
        "    if prediction == 1:\n",
        "        # 사람 감지 시 빨간색으로 시각화\n",
        "        color = [1, 0, 0]  # Red\n",
        "    else:\n",
        "        # 사람 미감지 시 파란색으로 시각화\n",
        "        color = [0, 0, 1]  # Blue\n",
        "\n",
        "    pcd.paint_uniform_color(color)  # 전체 포인트 클라우드 색상 변경\n",
        "\n",
        "    # 시각화\n",
        "    o3d.visualization.draw_geometries([pcd])\n",
        "\n",
        "# ========== mAP 계산 함수 ========== #\n",
        "def compute_map(h5_file, model_save_path):\n",
        "    # HDF5 파일 로드\n",
        "    with h5py.File(h5_file, \"r\") as hf:\n",
        "        data = hf[\"data\"][:]\n",
        "        labels = hf[\"label\"][:]\n",
        "\n",
        "    # 데이터 준비\n",
        "    data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    dataset = TensorDataset(data, labels)\n",
        "    test_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # 모델 로드\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = PointNet(num_classes=2).to(device)\n",
        "    model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 예측값 및 실제값 저장\n",
        "    all_targets = []\n",
        "    all_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            scores = F.softmax(outputs, dim=1)[:, 1]  # \"Human\" 클래스의 확률\n",
        "\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_scores.extend(scores.cpu().numpy())\n",
        "\n",
        "    # Precision-Recall Curve 계산\n",
        "    precision, recall, _ = precision_recall_curve(all_targets, all_scores)\n",
        "    ap = average_precision_score(all_targets, all_scores)\n",
        "\n",
        "    # 그래프 그리기\n",
        "    plt.figure()\n",
        "    plt.step(recall, precision, where='post', label=f\"AP={ap:.4f}\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Average Precision (AP): {ap:.4f}\")\n",
        "    return ap\n",
        "\n",
        "\n",
        "# ========== 실행 코드 ========== #\n",
        "if __name__ == \"__main__\":\n",
        "    # HDF5 데이터 생성\n",
        "    create_hdf5_from_pcd(processed_pcd_folder, output_h5_file)\n",
        "\n",
        "    # 모델 학습\n",
        "    train_pointnet(output_h5_file, model_save_path)\n",
        "\n",
        "    # 모델 평가\n",
        "    evaluate_model(output_h5_file, model_save_path)\n",
        "\n",
        "    # mAP 계산\n",
        "    compute_map(output_h5_file, model_save_path)\n",
        "\n",
        "    # 테스트 실행\n",
        "    test_pointnet(model_save_path, test_pcd_file)"
      ],
      "metadata": {
        "id": "jO9BWOQb4HVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "70sqHzG9K1sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. DETR 모델 로드 (ResNet-50 기반 pretrained 모델)\n",
        "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# COCO 데이터셋 클래스 (DETR이 COCO 데이터셋으로 학습됨)\n",
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "    'traffic light', 'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
        "    'dog'\n",
        "]\n"
      ],
      "metadata": {
        "id": "nAffZUlkBlyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 이미지 전처리 함수\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),  # 이미지 크기를 800px로 조정\n",
        "    T.ToTensor(),  # 텐서 변환\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 정규화\n",
        "])"
      ],
      "metadata": {
        "id": "B0QKdhV8BlPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 사람 탐지 함수 정의\n",
        "def detect_person(image_path):\n",
        "    # 이미지 로드 및 전처리\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    # 모델 추론\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "\n",
        "    # 출력에서 클래스 및 바운딩 박스 정보 추출\n",
        "    logits = outputs['pred_logits'][0]  # [N, num_classes]\n",
        "    boxes = outputs['pred_boxes'][0]    # [N, 4]\n",
        "\n",
        "    # Softmax를 사용해 클래스 확률 계산\n",
        "    prob = logits.softmax(-1)\n",
        "    labels = prob[..., :-1].argmax(-1)  # 마지막 클래스는 'no object'\n",
        "    scores = prob.max(-1).values\n",
        "\n",
        "    # \"person\" 클래스에 해당하는 바운딩 박스만 필터링\n",
        "    person_boxes = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if label == CLASSES.index('person') and scores[i] > 0.7:  # 신뢰도 임계값 0.7\n",
        "            box = boxes[i].cpu().numpy()\n",
        "            person_boxes.append((box, scores[i].item()))\n",
        "\n",
        "    return image, person_boxes"
      ],
      "metadata": {
        "id": "TI6OPCNTBrQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 바운딩 박스 시각화 함수\n",
        "def plot_results(image, person_boxes):\n",
        "    # 원본 이미지를 NumPy 배열로 변환\n",
        "    image_np = np.array(image)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(image_np)\n",
        "\n",
        "    # 바운딩 박스 그리기\n",
        "    for box, score in person_boxes:\n",
        "        # 바운딩 박스는 정규화된 좌표값으로 제공되므로, 이미지 크기에 맞게 변환\n",
        "        h, w = image_np.shape[:2]\n",
        "        xmin, ymin, xmax, ymax = box * np.array([w, h, w, h])\n",
        "\n",
        "        # 바운딩 박스 그리기\n",
        "        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                          fill=False, edgecolor='red', linewidth=2))\n",
        "        plt.text(xmin, ymin, f\"Person: {score:.2f}\", color='white',\n",
        "                 bbox=dict(facecolor='red', edgecolor='none', pad=2))\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ArZA4BwvBrN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 이미지 파일 경로 설정\n",
        "image_path = \"path_to_your_image.jpg\"  # 테스트할 도로 주행 이미지 경로"
      ],
      "metadata": {
        "id": "yWaorpXzBrLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 사람 탐지 및 결과 시각화\n",
        "image, person_boxes = detect_person(image_path)\n",
        "if person_boxes:\n",
        "    print(f\"Detected {len(person_boxes)} person(s) in the image.\")\n",
        "    plot_results(image, person_boxes)\n",
        "else:\n",
        "    print(\"No person detected.\")"
      ],
      "metadata": {
        "id": "2D8-ZIaoBrJJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}